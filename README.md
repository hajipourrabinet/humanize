Certainly! Below is a comprehensive `README.md` file tailored to your project, which includes scripts for **humanizing text** and **detecting AI-generated content**. This README provides an overview, installation instructions, usage guidelines, and additional information to help users get started effectively.

---

# Text Humanization and AI Detection Tool

## Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Usage](#usage)
  - [1. Humanize Text](#1-humanize-text)
  - [2. Detect AI-Generated Text](#2-detect-ai-generated-text)
- [Project Structure](#project-structure)
- [Dependencies](#dependencies)
- [Additional Notes](#additional-notes)
- [Troubleshooting](#troubleshooting)
- [Contributing](#contributing)
- [License](#license)

---

## Overview

This project comprises two primary Python scripts designed to enhance and analyze textual content:

1. **Humanize Text**: Enhances the naturalness of text by replacing common phrases with more human-like alternatives, substituting words with synonyms, and restructuring sentences to improve readability and variety.

2. **AI Detection**: Utilizes a pre-trained transformer model to classify text as either **AI-generated** or **Human-written**, providing confidence scores for each classification.

These tools are particularly useful for content creators, educators, and developers aiming to produce more engaging text and verify the originality of content.

---

## Features

- **Phrase Replacement**: Substitutes overused or formal phrases with more conversational alternatives.
- **Synonym Replacement**: Replaces words with context-appropriate synonyms to add variety.
- **Sentence Restructuring**: Shuffles and restructures sentences to enhance flow and readability.
- **AI Detection**: Classifies text based on whether it is generated by AI or written by a human, leveraging advanced NLP models.
- **Customization**: Easily extendable phrase maps and synonym databases for tailored text processing.

---

## Prerequisites

Before you begin, ensure you have met the following requirements:

- **Operating System**: Windows, macOS, or Linux.
- **Python**: Version 3.8 or higher.
- **Internet Connection**: Required for downloading models and datasets during the initial setup.

---

## Installation

Follow these steps to set up the project on your local machine:

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/text-humanization-ai-detection.git
cd text-humanization-ai-detection
```

*Replace `yourusername` with your actual GitHub username if applicable.*

### 2. Create a Virtual Environment (Optional but Recommended)

Creating a virtual environment helps manage dependencies and avoid conflicts.

```bash
python -m venv venv
```

Activate the virtual environment:

- **Windows**:

  ```bash
  venv\Scripts\activate
  ```

- **macOS/Linux**:

  ```bash
  source venv/bin/activate
  ```

### 3. Install Dependencies

Use the provided `requirements.txt` to install all necessary Python packages.

```bash
pip install -r requirements.txt
```

### 4. Download spaCy's English Model

Ensure that spaCy's English model is installed for advanced NLP tasks.

```bash
python -m spacy download en_core_web_sm
```

*Note: The scripts include automated checks to download necessary models and NLTK data. However, running the above command manually can prevent potential issues.*

---

## Usage

The project consists of two main scripts:

1. `humanize_text.py`: Enhances and humanizes input text.
2. `detect_ai.py`: Analyzes text to determine if it's AI-generated or human-written.

### 1. Humanize Text

**Purpose**: Replace common phrases and words with more natural alternatives and restructure sentences for improved readability.

**Steps**:

1. **Prepare Input File**: Create a text file named `input_text.txt` in the project directory containing the text you want to humanize.

2. **Run the Script**:

   ```bash
   python humanize_text.py
   ```

3. **Output**: The humanized text will be saved to `humanized_text.txt` in the same directory.

**Example**:

```plaintext
Original text from file: I am writing to you due to the fact that I need assistance as soon as possible.
Highly humanized text saved to: I'm reaching out to you because I need help at your earliest convenience.
```

### 2. Detect AI-Generated Text

**Purpose**: Classify the provided text as either AI-generated or human-written, along with confidence scores.

**Steps**:

1. **Ensure Humanized Text Exists**: Make sure `humanized_text.txt` is present in the project directory. If not, run the `humanize_text.py` script first.

2. **Run the Script**:

   ```bash
   python detect_ai.py
   ```

3. **Output**: The script will display the detection result and corresponding confidence scores.

**Example**:

```plaintext
Detection result: Human-written
Human-written Score: 0.85
AI-generated Score: 0.15
```

---

## Project Structure

Your project directory should resemble the following structure:

```
text-humanization-ai-detection/
├── humanize_text.py          # Script for humanizing text
├── detect_ai.py              # Script for detecting AI-generated text
├── input_text.txt            # Input file for humanization
├── humanized_text.txt        # Output from humanization and input for AI detection
├── requirements.txt          # Python dependencies
├── README.md                 # Project documentation
└── .gitignore                # (Optional) Git ignore file
```

*Ensure that both `humanize_text.py` and `detect_ai.py` are present in the root directory along with the input and output text files.*

---

## Dependencies

All necessary Python packages are listed in the `requirements.txt` file. Below is a brief overview of each dependency:

- **nltk (>=3.7)**: Natural Language Toolkit for text processing tasks such as tokenization and synonym retrieval.
- **spacy (>=3.5.0)**: Advanced NLP library for tasks like POS tagging and dependency parsing.
- **transformers (>=4.30.0)**: Library for state-of-the-art NLP models, used here for AI detection.
- **torch (>=2.0.1)**: PyTorch deep learning framework required by `transformers`.
- **regex**: Used for advanced text manipulation.
- **other dependencies**: Any additional dependencies required by the scripts.

*The `requirements.txt` file should contain:*

```plaintext
nltk>=3.7
spacy>=3.5.0
transformers>=4.30.0
torch>=2.0.1
```

*Additional dependencies like `regex` are handled within the scripts or can be added if necessary.*

---

## Additional Notes

- **NLTK Data**: The scripts automatically download necessary NLTK datasets (`wordnet`, `omw-1.4`, `punkt`, and `averaged_perceptron_tagger`) when run for the first time. Ensure an active internet connection during this initial run.

- **Model Selection**: The AI detection script uses the `roberta-base` model by default. For improved accuracy, consider using a model fine-tuned specifically for AI text detection. You can replace `model_name = "roberta-base"` with a more specialized model from Hugging Face's model hub.

- **Customization**: 
  - **Phrase Map**: Expand or modify the `phrase_map` dictionary in `humanize_text.py` to include more phrases tailored to your needs.
  - **Synonym Replacement Probability**: Adjust the probability thresholds in the scripts to control the extent of text modifications.

- **Performance**: For large texts, processing time may increase. Consider optimizing scripts or processing texts in smaller chunks if necessary.

---

## Troubleshooting

- **Module Not Found Errors**: Ensure all dependencies are installed correctly by running `pip install -r requirements.txt`.

- **spaCy Model Issues**: If you encounter errors related to the spaCy model, manually download it using:

  ```bash
  python -m spacy download en_core_web_sm
  ```

- **Model Loading Errors**: Verify that the Hugging Face model (`roberta-base` or your chosen model) is correctly specified and that your internet connection is stable for downloading models.

- **Insufficient Memory**: Large models like `roberta-base` may require significant memory. Ensure your system meets the necessary requirements or opt for smaller models if needed.

---

## Contributing

Contributions are welcome! If you have suggestions, bug reports, or enhancements, please open an issue or submit a pull request.

1. **Fork the Repository**

2. **Create a New Branch**

   ```bash
   git checkout -b feature/YourFeatureName
   ```

3. **Commit Your Changes**

   ```bash
   git commit -m "Add your message here"
   ```

4. **Push to the Branch**

   ```bash
   git push origin feature/YourFeatureName
   ```

5. **Open a Pull Request**

---

## License

This project is licensed under the [MIT License](LICENSE). Feel free to use, modify, and distribute as per the license terms.

---

## Acknowledgments

- **NLTK**: For providing robust tools for natural language processing.
- **spaCy**: For advanced NLP capabilities.
- **Hugging Face Transformers**: For offering state-of-the-art models and easy-to-use APIs.
- **PyTorch**: For the underlying deep learning framework that powers the models.

---

Feel free to reach out if you have any questions or need further assistance!